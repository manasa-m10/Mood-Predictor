# -*- coding: utf-8 -*-
"""wellnessApp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wt7cMaGb6erbqv5lcJRMFDa-q32QYSw6
"""

#!pip install -U transformers --quiet

#!pip install -q transformers datasets scikit-learn

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

DATA_DIR = "/content/drive/MyDrive/archive/data"

with open(f"{DATA_DIR}/emotions.txt", "r") as f:
    emotions = [line.strip() for line in f.readlines()]

columns = ["text", "labels", "id"]
train_df = pd.read_csv(f"{DATA_DIR}/train.tsv", sep="\t", names=columns)
test_df = pd.read_csv(f"{DATA_DIR}/test.tsv", sep="\t", names=columns)
dev_df = pd.read_csv(f"{DATA_DIR}/dev.tsv", sep="\t", names=columns)

df = pd.concat([train_df, test_df, dev_df])

emotion_to_mood = {
    "amusement": "happy", "approval": "happy", "caring": "calm", "confusion": "neutral", "curiosity": "neutral",
    "desire": "neutral", "excitement": "happy", "gratitude": "happy", "joy": "happy", "love": "happy",
    "optimism": "happy", "pride": "happy", "relief": "happy", "admiration": "happy",
    "anger": "angry", "annoyance": "angry", "disappointment": "sad", "disapproval": "angry",
    "embarrassment": "sad", "fear": "fearful", "grief": "sad", "nervousness": "fearful", "remorse": "sad",
    "sadness": "sad", "realization": "neutral", "surprise": "happy", "neutral": "neutral"
}

def get_first_mood(label_str):
    try:
        label_indices = list(map(int, label_str.split(',')))
        emotion = emotions[label_indices[0]]
        return emotion_to_mood.get(emotion, None)
    except:
        return None

df['mood'] = df['labels'].apply(get_first_mood)
df = df[df['mood'].notnull()].reset_index(drop=True)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['label'] = le.fit_transform(df['mood'])

label_map = dict(zip(le.classes_, le.transform(le.classes_)))
print(label_map)

from sklearn.model_selection import train_test_split

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(),
    df['label'].tolist(),
    test_size=0.2,
    stratify=df['label'],
    random_state=42
)

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)

import torch

class MoodDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __getitem__(self, idx):
        return {
            key: torch.tensor(val[idx]) for key, val in self.encodings.items()
        } | {'labels': torch.tensor(self.labels[idx])}
    def __len__(self):
        return len(self.labels)

train_dataset = MoodDataset(train_encodings, train_labels)
val_dataset = MoodDataset(val_encodings, val_labels)

#!pip install -U transformers datasets

#!pip install -U transformers datasets --quiet

import transformers
print(transformers.__version__)

from huggingface_hub import login
login()

from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score


le = LabelEncoder()
df['label'] = le.fit_transform(df['mood'])

model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased", num_labels=len(le.classes_)
)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc
    }

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=50,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()

trainer.evaluate()

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
label_encoder.fit(df["mood"])

def predict_mood(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128).to(model.device)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred_class = torch.argmax(probs, dim=1).item()
    return label_encoder.inverse_transform([pred_class])[0]

print(predict_mood("I feel enthusiastic"))

# Save the fine-tuned model
model_path = "./mood_classifier"
trainer.save_model(model_path)  # Saves the model and tokenizer
tokenizer.save_pretrained(model_path)

# Also save the LabelEncoder (you'll need it to decode predictions later)
import joblib
joblib.dump(le, f"{model_path}/label_encoder.pkl")

!ls /content/drive/MyDrive/archive

model_path = "/content/drive/MyDrive/archive/mood_classifier"
trainer.save_model(model_path)
tokenizer.save_pretrained(model_path)

import joblib
joblib.dump(le, f"{model_path}/label_encoder.pkl")